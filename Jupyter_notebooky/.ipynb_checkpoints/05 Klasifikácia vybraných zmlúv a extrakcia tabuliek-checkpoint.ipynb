{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomocou nasledujúceho skriptu vieme klasifikovať textové zmluvy na základe kľúčových slov zo súboru **keywords.txt**, výstupom bude tabuľka obsahujúca k metadátam aj údaje o jednotlivom výskyte kľúčových slov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "find_txt    = re.compile('txt')\n",
    "working_dir = os.getcwd()+'\\\\contracts_text\\\\'\n",
    "\n",
    "extract_ID = re.compile(r'\\d+')\n",
    "\n",
    "contracts = [f for f in os.listdir(working_dir) if os.path.isfile(os.path.join(working_dir, f))]\n",
    "contracts_txt = [f for f in contracts if (len(find_txt.findall(f))>0)]\n",
    "\n",
    "# Import clean table to extract metadata from it\n",
    "DB_clean = pd.read_csv('CRZ_DB_clean.csv', delimiter = '|')\n",
    "\n",
    "# Keywords are stored in keywords.txt provided in rows as categories separated by comma, first word is name of the category\n",
    "# Script searches for keywords, keywords are prepared by lowercasing\n",
    "\n",
    "fo = open('keywords.txt', 'r', encoding = 'utf-8')\n",
    "lines = fo.readlines()\n",
    "fo.close()\n",
    "\n",
    "categories = []\n",
    "\n",
    "# Import keywords from keywords.txt and prepare data structure\n",
    "for line in lines:\n",
    "    line = line.split(',')\n",
    "\n",
    "    category_name = line[0]\n",
    "    keywords = []\n",
    "    hits     = []\n",
    "    hits_per_category = 0\n",
    "\n",
    "    for i, item in enumerate(line):\n",
    "        if (i>0):\n",
    "            keywords.append(item.strip().casefold())\n",
    "            hits.append(0)\n",
    "\n",
    "    categories.append([category_name,keywords,hits,hits_per_category])\n",
    "\n",
    "# Prepare header for export\n",
    "header_metadata = ['Nazov','ID','Inner-ID','Objednavatel_ICO','Objednavatel','Objednavatel_adresa','Dodavatel_ICO','Dodavatel','Dodavatel_adresa',\n",
    "                   'Datum_zverejnenia','Datum_podpisu','Datum_platnosti','Datum_ucinnosti','Posledna_zmena','Cena_konecna','Cena_podpisana','Rezort','Typ',\n",
    "                   'Priloha_ID','Priloha_nazov','Link','Velkost','Datum','Text']\n",
    "\n",
    "header_sum_cat    = ['Výskyty']\n",
    "header_categories = [category[0] for category in categories]\n",
    "header_keywords   = []\n",
    "\n",
    "for category in categories:\n",
    "    header_keywords = header_keywords + category[1]\n",
    "\n",
    "header = header_metadata + header_sum_cat + header_categories + header_keywords\n",
    "\n",
    "row_list = []\n",
    "N = len(contracts_txt)\n",
    "\n",
    "number_of_characters = []\n",
    "\n",
    "# Go through all processed text files, lowercase it, for every keyword count number of occurrences\n",
    "for i, contract in enumerate(contracts_txt):\n",
    "    print('Analysing contract: ', contract, ' ', i, 'out of', N)\n",
    "\n",
    "    fo = open(working_dir+contract, 'r', encoding = 'utf-8')\n",
    "    lines = fo.readlines()\n",
    "    fo.close()\n",
    "\n",
    "    text = ''\n",
    "\n",
    "    for line in lines:\n",
    "        text += line.casefold().replace('\\n',' ')\n",
    "\n",
    "    del lines\n",
    "\n",
    "    for category in categories:\n",
    "        category[3] = 0\n",
    "        for j, keyword in enumerate(category[1]):\n",
    "            category[2][j] = text.count(keyword.casefold())\n",
    "            category[3] += category[2][j]\n",
    "\n",
    "    # Extract metadata and join it with counted hits\n",
    "    row = DB_clean.loc[DB_clean['ID'] == int(extract_ID.findall(contract)[0])]\n",
    "\n",
    "    meta_data = [row.iloc[0,i] for i in range(1,19)]\n",
    "    attachment_data = ast.literal_eval(row.iloc[0,20])[0]\n",
    "\n",
    "    meta_data = meta_data + attachment_data\n",
    "    data_hits = []\n",
    "\n",
    "    for category in categories:\n",
    "        data_hits += category[2]\n",
    "\n",
    "    sum_data = 0\n",
    "    for category in categories:\n",
    "        sum_data += category[3]\n",
    "\n",
    "    data = meta_data + [sum_data] + [category[3] for category in categories] + data_hits\n",
    "    row_list.append(dict((label,data[i]) for i, label in enumerate(header)))\n",
    "\n",
    "    # Count number of characters in contract\n",
    "    number_of_characters.append(len(text))\n",
    "\n",
    "# Save unranked csv table\n",
    "DB_clean_tagged = pd.DataFrame(row_list, columns = header)\n",
    "DB_clean_tagged.to_csv('DB_clean_text_tagged.csv', header = header, sep='|')\n",
    "\n",
    "# Rank contracts according to number of keywords, number of characters in contract and price.\n",
    "# Ranking is based on three categories listed above, in each category 10 points are distributed\n",
    "# according to logarithmic scale and then added. Contracts are sorted by the rank.\n",
    "\n",
    "# Insert new column -- number of characters\n",
    "DB_clean_tagged.insert(24, 'Pocet_znakov', number_of_characters)\n",
    "DB_clean_tagged = DB_clean_tagged.sort_values(by=['Výskyty','Pocet_znakov','Cena_konecna'], ascending = False)\n",
    "\n",
    "# Sort rows by number of hits, number of characters and final prize\n",
    "delete_rows = []\n",
    "for index, row in DB_clean_tagged.iterrows():\n",
    "    if ((float(row['Pozícia']) == 0) and (float(row['Popis práce']) == 0)):\n",
    "        delete_rows.append(index)\n",
    "    if (float(row['Výskyty']) == 0):\n",
    "        delete_rows.append(index)\n",
    "\n",
    "print('Sorted : ',N,'| Filtered out : ',len(delete_rows))\n",
    "\n",
    "DB_clean_tagged = DB_clean_tagged.drop(delete_rows)\n",
    "DB_clean_tagged.to_csv('CRZ_DB_clean_text_tagged.csv', sep='|')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nasledujúci skript presunie relevantné prílohy do samostatného podpriečinku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "source_dir    = os.getcwd()+'\\\\contracts_text\\\\'\n",
    "direction_dir = os.getcwd()+'\\\\contracts_mandayrates\\\\'\n",
    "\n",
    "if not os.path.exists('contracts_mandayrates'):\n",
    "    os.makedirs('contracts_mandayrates')\n",
    "\n",
    "DB_clean_tagged = pd.read_csv('DB_clean_text_tagged.csv', delimiter = '|')\n",
    "\n",
    "for index, row in DB_clean_tagged.iterrows():\n",
    "    contract = str(row['ID']) + '_' + str(row['Priloha_ID'])\n",
    "    os.system('move ' +source_dir+contract +'.pdf ' + direction_dir+contract +'.pdf')\n",
    "    os.system('move ' +source_dir+contract +'.txt ' + direction_dir+contract +'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import camelot\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# pdfminer for extracting information about number of pages\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import resolve1\n",
    "\n",
    "working_dir = os.getcwd()+'/contracts_mandayrates/'\n",
    "\n",
    "# Open tagged spreadsheet and read information, sorted\n",
    "DB_clean_tagged = pd.read_csv('CRZ_DB_clean_text_tagged.csv', delimiter = '|')\n",
    "DB_clean_tagged = DB_clean_tagged.drop('Unnamed: 0', 1)\n",
    "\n",
    "number_of_contracts = len(DB_clean_tagged.index)\n",
    "\n",
    "# If column with number of PDF pages in contract do not exist\n",
    "# then calculate it ...\n",
    "if not 'Pocet_stran' in DB_clean_tagged.columns:\n",
    "    DB_clean_tagged.insert(25, 'Pocet_stran', np.zeros(number_of_contracts))\n",
    "\n",
    "    # Identify how many pages are in every contract\n",
    "    # useful to estimate time needed to extract all tables from PDF\n",
    "    print('Counting pages in PDFs ...')\n",
    "    i = 0\n",
    "    for index, row in DB_clean_tagged.iterrows():\n",
    "        i += 1\n",
    "\n",
    "        contract = str(row['ID']) + '_' + str(row['Priloha_ID']) + '.pdf'\n",
    "        print('Processing contract ', contract,' ', i, 'out of', number_of_contracts)\n",
    "\n",
    "        contract_file = open(working_dir + contract, 'rb')\n",
    "    \n",
    "        parser = PDFParser(contract_file)\n",
    "        document = PDFDocument(parser)\n",
    "\n",
    "        pages = resolve1(document.catalog['Pages'])['Count']\n",
    "        DB_clean_tagged.at[index,'Pocet_stran'] = int(pages)\n",
    "\n",
    "        contract_file.close()\n",
    "\n",
    "    # Save partial result\n",
    "    DB_clean_tagged.to_csv('CRZ_DB_clean_text_tagged.csv', sep='|')\n",
    "\n",
    "sys.stdout.flush()\n",
    "# Analyse PDF pages and extract tables\n",
    "if not 'Pocet_tabuliek' in DB_clean_tagged.columns:\n",
    "\n",
    "    total_pages = 0\n",
    "    # Calculate total number of pages to analyse\n",
    "    for index, row in DB_clean_tagged.iterrows():\n",
    "        total_pages += int(row['Pocet_stran'])\n",
    "\n",
    "    print('Total number of pages to analyse:', total_pages)\n",
    "\n",
    "    DB_clean_tagged.insert(25, 'Pocet_tabuliek', np.zeros(number_of_contracts))\n",
    "\n",
    "    empty_column = [None] * number_of_contracts\n",
    "    if not 'Tabulky_strany' in DB_clean_tagged.columns:\n",
    "        DB_clean_tagged.insert(26, 'Tabulky_strany', empty_column)\n",
    "\n",
    "    # Analyse PDF pages and extract tables\n",
    "    i = 0\n",
    "    for index, row in DB_clean_tagged.iterrows():\n",
    "        i += 1\n",
    "\n",
    "        contract = str(row['ID']) + '_' + str(row['Priloha_ID']) + '.pdf'\n",
    "        contract_dir = working_dir + contract.replace('.pdf','_tables')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if not os.path.exists(contract_dir):\n",
    "            os.makedirs(contract_dir)\n",
    "\n",
    "            number_of_pages = int(row['Pocet_stran'])\n",
    "\n",
    "            print('Processing contract ', contract,' ', i, 'out of', number_of_contracts)\n",
    "            print('Going to process', number_of_pages, 'pages ...')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            number_of_tables = 0\n",
    "            tables_pages = []\n",
    "\n",
    "            start_time = time.time()\n",
    "            for page in range(1,number_of_pages):\n",
    "\n",
    "                print('\\tProcessing page', page, 'out of', number_of_pages)\n",
    "                tables = camelot.read_pdf(working_dir + contract, pages = str(page))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                if (len(tables)>0):\n",
    "\n",
    "                    for j in range(0,len(tables)):\n",
    "                        tables[j].to_csv(contract_dir + '/table_' + str(number_of_tables + j + 1) + '.csv')\n",
    "                        tables_pages.append(page)\n",
    "                        print(tables[j].parsing_report)\n",
    "\n",
    "                    number_of_tables += len(tables)\n",
    "\n",
    "            end_time = time.time()\n",
    "            print('Processed ', number_of_pages, ' pages in ',(end_time-start_time))\n",
    "\n",
    "            DB_clean_tagged.at[index,'Pocet_tabuliek'] = int(number_of_tables)\n",
    "            DB_clean_tagged.at[index,'Tabulky_strany'] = str(tables_pages)\n",
    "\n",
    "            # Save information about number of pages\n",
    "            DB_clean_tagged.to_csv('CRZ_DB_clean_text_tagged.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import ast\n",
    "import shutil\n",
    "\n",
    "def natural_sort(l):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
    "    return sorted(l, key = alphanum_key)\n",
    "\n",
    "find_txt    = re.compile('txt')\n",
    "working_dir = os.getcwd()+'\\\\contracts_mandayrates\\\\'\n",
    "final_dir   = os.getcwd()+'\\\\contracts_mandayrates_tables\\\\'\n",
    "extract_number = re.compile(r'\\d+')\n",
    "\n",
    "# Check if there is directory _tables if yes, delete it, if no create it\n",
    "if os.path.isdir(final_dir):\n",
    "    shutil.rmtree(final_dir)\n",
    "else:\n",
    "    os.mkdir(final_dir)\n",
    "\n",
    "# List all subdirectories with tables in working dir\n",
    "subdirectories = [ndir for ndir in os.listdir(working_dir) if os.path.isdir(os.path.join(working_dir, ndir))]\n",
    "\n",
    "# Import file with keywords\n",
    "fo = open('keywords.txt', 'r', encoding = 'utf-8')\n",
    "lines = fo.readlines()\n",
    "fo.close()\n",
    "\n",
    "categories = []\n",
    "\n",
    "# Import keywords from keywords.txt and prepare data structures\n",
    "for line in lines:\n",
    "    line = line.split(',')\n",
    "\n",
    "    category_name = line[0]\n",
    "    keywords = []\n",
    "    hits     = []\n",
    "    hits_per_category = 0\n",
    "\n",
    "    for i, item in enumerate(line):\n",
    "        if (i>0):\n",
    "            keywords.append(item.strip())\n",
    "            hits.append(0)\n",
    "\n",
    "\tcategories.append([category_name,keywords,hits,hits_per_category])\n",
    "\n",
    "header_categories = [category[0] for category in categories]\n",
    "header_keywords   = []\n",
    "\n",
    "# Import metadata from text_tagged file\n",
    "DB_import = pd.read_csv('DB_clean_text_tagged.csv', delimiter = '|')\n",
    "DB_import = DB_import.drop('Unnamed: 0', 1)\n",
    "\n",
    "header_import = ['Nazov','ID','Inner-ID','Objednavatel_ICO','Objednavatel','Objednavatel_adresa','Dodavatel_ICO','Dodavatel','Dodavatel_adresa',\n",
    "\t\t\t\t'Datum_zverejnenia','Datum_podpisu','Datum_platnosti','Datum_ucinnosti','Posledna_zmena','Cena_konecna','Cena_podpisana','Rezort','Typ',\n",
    "\t\t\t\t'Priloha_ID','Priloha_nazov','Link','Velkost','Datum','Text','Pocet_znakov','Pocet_stran','Pocet_tabuliek','Tabulky_strany']\n",
    "\n",
    "header_metadata = ['Nazov','ID','Inner-ID','Objednavatel_ICO','Objednavatel','Objednavatel_adresa','Dodavatel_ICO','Dodavatel','Dodavatel_adresa',\n",
    "\t\t\t\t'Datum_zverejnenia','Datum_podpisu','Datum_platnosti','Datum_ucinnosti','Posledna_zmena','Cena_konecna','Cena_podpisana','Rezort','Typ',\n",
    "\t\t\t\t'Priloha_ID','Priloha_nazov','Link','Velkost','Datum','Text','Pocet_znakov','Pocet_stran','Tabulka_strana','Tabulka_cislo']\n",
    "\n",
    "len_header_import = len(header_import)\n",
    "DB_import = DB_import.drop(DB_import.columns.difference(header_import), axis=1)\n",
    "\n",
    "# Produce new header for new file\n",
    "header_sum_cat    = ['Výskyty']\n",
    "header_categories = [category[0] for category in categories]\n",
    "header_keywords   = []\n",
    "\n",
    "for category in categories:\n",
    "\theader_keywords = header_keywords + category[1]\n",
    "\n",
    "header = header_metadata + header_sum_cat + header_categories + header_keywords\n",
    "\n",
    "# For each CSV table in each subdirectory tag tables\n",
    "# Produce another CSV in which each row contain (meta)information about some table\n",
    "\n",
    "N_dir = len(subdirectories)\n",
    "row_list = []\n",
    "\n",
    "for index, directory in enumerate(subdirectories):\n",
    "\tprint('Processing contract', directory.strip('_tables'), '-', index+1, 'out of', N_dir)\n",
    "\n",
    "\ttable_dir = os.path.join(working_dir, directory)\n",
    "\ttables = [f for f in os.listdir(table_dir) if os.path.isfile(os.path.join(table_dir, f))]\n",
    "\n",
    "\t# Sort tables according to number in table_number.csv\n",
    "\ttables = natural_sort(tables)\n",
    "\n",
    "\tfor table in tables:\n",
    "\t\tfo = open(os.path.join(table_dir,table), 'r', encoding = 'utf-8')\n",
    "\t\tlines = fo.readlines()\n",
    "\t\tfo.close()\n",
    "\n",
    "\t\ttext = ''\n",
    "\n",
    "\t\tfor line in lines:\n",
    "\t\t\ttext += line.casefold().replace('\\n',' ')\n",
    "\n",
    "\t\tdel lines\n",
    "\n",
    "\t\tfor category in categories:\n",
    "\t\t\tcategory[3] = 0\n",
    "\t\t\tfor j, keyword in enumerate(category[1]):\n",
    "\t\t\t\tcategory[2][j] = text.count(keyword.casefold())\n",
    "\t\t\t\tcategory[3]   += category[2][j]\n",
    "\n",
    "\t\t# Extract metadata and join it with counted hits\n",
    "\t\trow = DB_import.loc[DB_import['Priloha_ID'] == int(extract_number.findall(table_dir)[1])]\n",
    "\n",
    "\t\tmeta_data = [row.iat[0,i] for i in range(0,len(header_import)-1)]\n",
    "\t\tmeta_data.append(int(extract_number.findall(table)[0]))\n",
    "\n",
    "\t\t# Insert number of the page\n",
    "\t\tif (float(row['Pocet_tabuliek'])>0):\n",
    "\t\t\tmeta_data[len(meta_data)-2] = ast.literal_eval(meta_data[len(meta_data)-2])[int(extract_number.findall(table)[0])-1]\n",
    "\t\telse:\n",
    "\t\t\tmeta_data[len(meta_data)-2] = 0\n",
    "\n",
    "\t\tdata_hits = []\n",
    "\n",
    "\t\tfor category in categories:\n",
    "\t\t\tdata_hits += category[2]\n",
    "\n",
    "\t\tsum_data = 0\n",
    "\t\tfor category in categories:\n",
    "\t\t\tsum_data += category[3]\n",
    "\n",
    "\t\tdata = meta_data + [sum_data] + [category[3] for category in categories] + data_hits\n",
    "\t\trow_list.append(dict((label,data[i]) for i, label in enumerate(header)))\n",
    "\n",
    "# Save unranked CSV table\n",
    "DB_export = pd.DataFrame(row_list, columns = header)\n",
    "DB_export.to_csv('CRZ_DB_clean_tables.csv', header = header, sep='|')\n",
    "\n",
    "# Filter out all irrelevant tables\n",
    "# and produce CSV which has only tables with at least one position and one\n",
    "delete_rows = []\n",
    "for index, row in DB_export.iterrows():\n",
    "\tif not(((float(row['Pozícia']) > 0) or (float(row['Popis práce']) > 0)) and (float(row['Kvantifikátor']) > 0)):\n",
    "\t\tdelete_rows.append(index)\n",
    "\n",
    "print('Number of tables : ', DB_export.shape[0],'| Filtered out : ', len(delete_rows))\n",
    "\n",
    "DB_export = DB_export.drop(delete_rows)\n",
    "DB_export.to_csv('CRZ_DB_clean_tables.csv', sep='|')\n",
    "\n",
    "# Copy all relevant tables into directory _tables\n",
    "for index, row in DB_export.iterrows():\n",
    "\tsource      = os.path.join(working_dir,str(row['ID']) + '_' + str(row['Priloha_ID']) + '_tables\\\\table_' + str(row['Tabulka_cislo']) + '.csv')\n",
    "\tdestination = os.path.join(final_dir,str(row['ID']) + '_' + str(row['Priloha_ID']) + '_' + str(row['Tabulka_cislo']) + '.csv')\n",
    "\tos.system('copy '+source+' '+destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matej Badin | UHP | 2019                                             |\n",
    "# -------------------------------------------------------------------- |\n",
    "# Packages needed :  numpy, re, os, pandas                             |\n",
    "# -------------------------------------------------------------------- |\n",
    "# Tag and filter extracted tables based on keywords                    |\n",
    "# -------------------------------------------------------------------- |\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import ast\n",
    "import operator\n",
    "import shutil\n",
    "\n",
    "def parse_text(text):\n",
    "\tslovak_alphabet = 'aáäbcčdďeéfghiíjklĺľmnňoóôpqrŕsštťuúvwxyýzž'\n",
    "\n",
    "\ttext = text.casefold()\n",
    "\twords = []\n",
    "\n",
    "\tnew_word = ''\n",
    "\tword = True\n",
    "\tfor char in text:\n",
    "\t\tif char in slovak_alphabet:\n",
    "\t\t\tnew_word = new_word + char\n",
    "\t\t\tword = True\n",
    "\t\telse:\n",
    "\t\t\tif word:\n",
    "\t\t\t\twords.append(new_word)\n",
    "\t\t\t\tnew_word = ''\n",
    "\t\t\tword = False\n",
    "\n",
    "\treturn words\n",
    "\n",
    "# Files\n",
    "working_dir = os.getcwd()\n",
    "tables_dir       = working_dir+'\\\\IT_contracts_mandayrates_tables'\n",
    "clean_tables_dir = working_dir+'\\\\IT_contracts_mandayrates_clean_tables'\n",
    "\n",
    "if os.path.isdir(clean_tables_dir):\n",
    "\tshutil.rmtree(clean_tables_dir)\n",
    "else:\n",
    "\tos.mkdir(clean_tables_dir)\n",
    "\n",
    "tables_csv = [f for f in os.listdir(tables_dir) if os.path.isfile(os.path.join(tables_dir, f))]\n",
    "\n",
    "# Import standard Slovak vocabulary corpus and dictionary\n",
    "import hunspell\n",
    "\n",
    "normal_SK  = os.path.join(working_dir, 'Dicts\\\\sk_SK')\n",
    "english_US = os.path.join(working_dir, 'Dicts\\\\en_US')\n",
    "special_SK = os.path.join(working_dir, 'Dicts\\\\sk_SK_special')\n",
    "\n",
    "# Dictionary with standard Slovak language and words from contracts in this sector by build_special_dictionary.py\n",
    "hunspell_normal  = hunspell.Hunspell(normal_SK, normal_SK)\n",
    "hunspell_english = hunspell.Hunspell(english_US, english_US)\n",
    "hunspell_special = hunspell.Hunspell(normal_SK, special_SK)\n",
    "\n",
    "# Own spellcheck function also making sure word is case-folded and whitespace is stripped\n",
    "def spell(word):\n",
    "\tword = word.casefold().strip()\n",
    "\treturn (hunspell_normal.spell(word) or hunspell_english.spell(word) or hunspell_special.spell(word))\n",
    "\n",
    "# Import keywords and add them to the special dictionary for spellchecking\n",
    "fo = open('keywords.txt', 'r', encoding = 'utf-8')\n",
    "lines = fo.readlines()\n",
    "fo.close()\n",
    "\n",
    "all_keywords  = []\n",
    "\n",
    "categories = []\n",
    "add_words = []\n",
    "\n",
    "# Import keywords from keywords.txt and put them inside special dictionary\n",
    "for line in lines:\n",
    "\tline = line.split(',')\n",
    "\n",
    "\tcategory_name = line[0]\n",
    "\tkeywords = []\n",
    "\n",
    "\tfor i, item in enumerate(line):\n",
    "\t\tif (i>0):\n",
    "\t\t\tkeywords.append(item.strip().casefold())\n",
    "\t\t\tall_keywords.append(item.strip().casefold())\n",
    "\n",
    "\tcategories.append([category_name,keywords])\n",
    "\n",
    "# Add keywords to special if they are wrong\n",
    "for keyword in all_keywords:\n",
    "\twords = keyword.split()\n",
    "\tfor word in words:\n",
    "\t\tif not spell(word):\n",
    "\t\t\tadd_words.append(word)\n",
    "\n",
    "# Copy special dictionary and append lines with keywords, reload hunspell_special\n",
    "special_dic_with_keywords = os.path.join(working_dir, 'Dicts\\\\sk_SK_special_with_keywords')\n",
    "# Copy file\n",
    "# Change number in first line\n",
    "# Append lines to file\n",
    "\n",
    "# Reload special Hunspell dictionary\n",
    "hunspell_special = hunspell.Hunspell(normal_SK, special_dic_with_keywords)\n",
    "\n",
    "# Empty dictionary to be filled with suggested keywords\n",
    "suggested_keywords = dict()\n",
    "\n",
    "#####################################################################################################\n",
    "# Analysis !\n",
    "#####################################################################################################\n",
    "\n",
    "N_tables = len(tables_csv)\n",
    "for i, table_csv in enumerate(tables_csv):\n",
    "\tprint('Processing table:',table_csv,' ', i+1, 'out of',N_tables)\n",
    "\n",
    "\t# Step 1\n",
    "\t# Read CSV and destroy any new line characters between \" characters\n",
    "\tfo = open(os.path.join(tables_dir,table_csv), 'r', encoding = 'utf-8')\n",
    "\tlines = fo.readlines()\n",
    "\tfo.close()\n",
    "\n",
    "\treminder = 0\n",
    "\n",
    "\ttext = ''\n",
    "\tfor line in lines:\n",
    "\t\tfor char in line:\n",
    "\t\t\tif char == '\"':\n",
    "\t\t\t\treminder += 1\n",
    "\t\t\treminder = reminder % 2\n",
    "\n",
    "\t\t\tif ((char == '\\n') and (reminder == 1)):\n",
    "\t\t\t\tpass\n",
    "\t\t\telse:\n",
    "\t\t\t\ttext += char\n",
    "\n",
    "\tfo = open(os.path.join(tables_dir,'temp.csv'), 'w', encoding = 'utf-8')\n",
    "\tfo.writelines(text)\n",
    "\tfo.close()\n",
    "\n",
    "\t# Step 2\n",
    "\t# Import CSV table into pandas and delete empty columns\n",
    "\ttable = pd.read_csv(os.path.join(tables_dir,'temp.csv'), delimiter = ',')\n",
    "\tempty = dict((column,True) for column in table.columns)\n",
    "\n",
    "\tfor column in table.columns:\n",
    "\t\tfor index, row in table.iterrows():\n",
    "\t\t\tif not((str(row[column]).rstrip() == '') or (str(row[column]) == 'nan')):\n",
    "\t\t\t\tempty[column] = False\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\tdelete = [column for column in table.columns if empty[column]]\n",
    "\ttable = table.drop(columns = delete, axis = 1)\n",
    "\n",
    "\t# Step 3\n",
    "\t# Try to identify columns with just dummy characters and not any meaningful word\n",
    "\t# and ... also destroy them\n",
    "\tdummy = dict((column,False) for column in table.columns)\n",
    "\tfor column in table.columns:\n",
    "\n",
    "\t\tcorrect = 0\n",
    "\t\twrong   = 0\n",
    "\n",
    "\t\tfor index, row in table.iterrows():\n",
    "\t\t\twords = str(row[column]).casefold().split()\n",
    "\t\t\tfor word in words:\n",
    "\t\t\t\tif spell(word):\n",
    "\t\t\t\t\tcorrect += 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\twrong += 1\n",
    "\n",
    "\t\t# Arbitrarily chosen number\n",
    "\t\tif (wrong/(wrong+correct)>0.75):\n",
    "\t\t\tdummy[column] = True\n",
    "\n",
    "\tdelete = [column for column in table.columns if dummy[column]]\n",
    "\ttable = table.drop(columns = delete, axis = 1)\n",
    "\t# Save clean table\n",
    "\ttable.to_csv(os.path.join(tables_dir,'temp.csv'), sep = ',')\n",
    "\n",
    "\t# Step 4\n",
    "\t# Identify if the first row is the header\n",
    "\theader = False\n",
    "\n",
    "\t# Select keywords in categories 'Hlavička tabuľky'\n",
    "\tselected_keywords   = []\n",
    "\tselected_categories = ['Hlavička tabuľky']\n",
    "\n",
    "\t# but still code in general ;)\n",
    "\tfor category in categories:\n",
    "\t\tif category[0] in selected_categories:\n",
    "\t\t\tfor keyword in category[1]:\n",
    "\t\t\t\tselected_keywords.append(keyword)\n",
    "\n",
    "\t# Pandas already tried to infer header from CSV - such nice from it ...\n",
    "\tnumber_of_hits = 0\n",
    "\tfor keyword in selected_keywords:\n",
    "\t\tfor column in table.columns:\n",
    "\t\t\tnumber_of_hits += column.count(keyword)\n",
    "\n",
    "\t# Arbitrarily chosen boundary\n",
    "\tif (number_of_hits>2):\n",
    "\t\theader = True\n",
    "\n",
    "\t# Step 5\n",
    "\t# Identify if there is a specific column with 'Pozicia'\n",
    "\tselected_keywords   = []\n",
    "\tselected_categories = ['Pozícia','Popis práce']\n",
    "\tnumber_of_hits      = dict((column,0) for column in table.columns)\n",
    "\n",
    "\tfor category in categories:\n",
    "\t\tif category[0] in selected_categories:\n",
    "\t\t\tfor keyword in category[1]:\n",
    "\t\t\t\tselected_keywords.append(keyword)\n",
    "\n",
    "\tfor column in table.columns:\n",
    "\t\tfor row in table[column]:\n",
    "\t\t\trow = str(row).casefold()\n",
    "\t\t\tfor keyword in selected_keywords:\n",
    "\t\t\t\tif keyword in row:\n",
    "\t\t\t\t\tnumber_of_hits[column] += 1\n",
    "\n",
    "\t# Sorted columns with 'Pozicia'-like keywords if number of hits is at least > 1\n",
    "\tpositions_columns = [(column,number_of_hits[column]/table.shape[0]) for column in table.columns if number_of_hits[column] > 0]\n",
    "\tpositions_columns = sorted(positions_columns, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "\t# Step 6\n",
    "\t# .. also try to identify columns which has significant number of rows containing numbers or prices\n",
    "\tfind_number = re.compile(r'\\d+')\n",
    "\tprice_header = ['']\n",
    "\n",
    "\tselected_keywords   = []\n",
    "\tselected_categories = ['Hlavička cena']\n",
    "\n",
    "\tfor category in categories:\n",
    "\t\tif category[0] in selected_categories:\n",
    "\t\t\tfor keyword in category[1]:\n",
    "\t\t\t\tselected_keywords.append(keyword)\n",
    "\n",
    "\tprices_columns = []\n",
    "\tif header:\n",
    "\t\tfor column in table.columns:\n",
    "\t\t\tfor keyword in selected_keywords:\n",
    "\t\t\t\tif keyword in column:\n",
    "\t\t\t\t\tif not column in prices_columns:\n",
    "\t\t\t\t\t\tprices_columns.append(column)\n",
    "\n",
    "\tratio_of_number_rows = dict((column,0) for column in table.columns)\n",
    "\tfor column in table.columns:\n",
    "\t\tfor row in table[column]:\n",
    "\t\t\trow = str(row)\n",
    "\t\t\tif len(find_number.findall(row))>0:\n",
    "\t\t\t\tratio_of_number_rows[column] += 1\n",
    "\n",
    "\t\tratio_of_number_rows[column] = ratio_of_number_rows[column]/table.shape[0]\n",
    "\n",
    "\tif len(prices_columns)>0:\n",
    "\t\tprices_columns = [(column,ratio_of_number_rows[column]) for column in prices_columns if ratio_of_number_rows[column] > 0.75]\n",
    "\telse:\n",
    "\t\tprices_columns = [(column,ratio_of_number_rows[column]) for column in table.columns if ratio_of_number_rows[column] > 0.75]\n",
    "\n",
    "\tpositions_columns_names = [column[0] for column in positions_columns]\n",
    "\tfor column in prices_columns:\n",
    "\t\tif column[0] in positions_columns_names:\n",
    "\t\t\tprices_columns.remove(column)\n",
    "\n",
    "\tprices_columns = sorted(prices_columns, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "\t# Suggest new keywords based on data in the rows identified as this\n",
    "\t#  new keywords are given points according to the relative number of rows which already contain\n",
    "\t#  some selected keyword.\n",
    "\tif ((len(positions_columns)>0) and (len(prices_columns)>0)):\n",
    "\t\tfor column in positions_columns:\n",
    "\t\t\tfor row in table[column[0]]:\n",
    "\t\t\t\trow = str(row).casefold()\n",
    "\t\t\t\twords = parse_text(row)\n",
    "\t\t\t\tfor word in words:\n",
    "\t\t\t\t\tif not word in selected_keywords:\n",
    "\t\t\t\t\t\tif word in suggested_keywords:\n",
    "\t\t\t\t\t\t\tsuggested_keywords[word] += column[1]\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tsuggested_keywords[word]  = column[1]\n",
    "\n",
    "\t# Save to clean directory only if there is at least a single price column\n",
    "\tif (len(prices_columns)>0):\n",
    "\t\ttable.to_csv(os.path.join(clean_tables_dir,table_csv), sep = ',')\n",
    "\n",
    "\t# Step 7\n",
    "\t# Identify in which column there is DPH or not\n",
    "\twith_DPH    = False\n",
    "\twithout_DPH = False\n",
    "\n",
    "\tfor column in table.columns:\n",
    "\t\tif 's DPH'   in column: with_DPH    = True\n",
    "\t\tif 'bez DPH' in column: without_DPH = True\n",
    "\n",
    "\t# Print metadata\n",
    "\tprint('Header:',header)\n",
    "\tprint('Positions:',positions_columns)\n",
    "\tprint('Prices:',prices_columns)\n",
    "\tprint('s DPH:',with_DPH)\n",
    "\tprint('bez DPH:',without_DPH)\n",
    "\n",
    "# Print suggested keywords\n",
    "suggested_keywords = sorted(suggested_keywords.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "fo = open('suggested_keywords.txt','w')\n",
    "for keyword in suggested_keywords:\n",
    "\tfo.write(keyword[0]+'\\t\\t\\t'+str(keyword[1])+'\\n')\n",
    "fo.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
